{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# KMeans Class\n",
        "class KMeans:\n",
        "    def train(self, hyperparameters, X, y=None):\n",
        "        # Validate and extract hyperparameters\n",
        "        K = hyperparameters.get('K')\n",
        "        tau = hyperparameters.get('tau')\n",
        "        max_iterations = hyperparameters.get('max_iterations')\n",
        "        if not isinstance(K, int) or K <= 0:\n",
        "            raise ValueError(\"Number of clusters K must be a positive integer.\")\n",
        "        if not isinstance(tau, (float, int)) or tau < 0:\n",
        "            raise ValueError(\"Convergence threshold tau must be a non-negative number.\")\n",
        "        if not isinstance(max_iterations, int) or max_iterations <= 0:\n",
        "            raise ValueError(\"Maximum iterations must be a positive integer.\")\n",
        "        # Get dataset dimensions\n",
        "        n_samples, n_features = X.shape\n",
        "        # Add a column for cluster assignments to X\n",
        "        X = np.hstack((X, np.zeros((n_samples, 1))))\n",
        "        # Initialize centroids (K x n_features)\n",
        "        centroids = np.random.rand(K, n_features)\n",
        "        # Initialize vector to count number of samples per cluster\n",
        "        cluster_sizes = np.zeros(K)\n",
        "\n",
        "        # Randomly assign initial clusters\n",
        "        for i in range(n_samples):\n",
        "            random_cluster = np.random.randint(0, K)\n",
        "            X[i, -1] = random_cluster\n",
        "            cluster_sizes[int(random_cluster)] += 1\n",
        "        # Boolean variable to check convergence\n",
        "        convergence = False\n",
        "        # Iterative loop for K-Means\n",
        "        for iteration in range(max_iterations):\n",
        "            # Backup the current centroids\n",
        "            prev_centroids = centroids.copy()\n",
        "            # Reset centroids and cluster sizes\n",
        "            centroids = np.zeros((K, n_features))\n",
        "            cluster_sizes = np.zeros(K)\n",
        "            # Accumulate feature sums for each cluster\n",
        "            for i in range(n_samples):\n",
        "                cluster = int(X[i, -1])\n",
        "                centroids[cluster] += X[i, :-1]\n",
        "                cluster_sizes[cluster] += 1\n",
        "            # Compute new centroids\n",
        "            for k in range(K):\n",
        "                if cluster_sizes[k] > 0:\n",
        "                    centroids[k] *= (1.0 / cluster_sizes[k])  # Average features\n",
        "\n",
        "            # Check for convergence\n",
        "            centroid_shift = np.linalg.norm(centroids - prev_centroids)\n",
        "            if centroid_shift < tau:\n",
        "                convergence = True\n",
        "                print(f\"Converged after {iteration + 1} iterations.\")\n",
        "                break\n",
        "\n",
        "        if not convergence:\n",
        "            print(f\"Reached maximum iterations: {max_iterations}.\")\n",
        "\n",
        "        return centroids, X[:, -1]\n",
        "\n",
        "# SemiSupervisedKMeans Class (inherits from KMeans)\n",
        "class SemiSupervisedKMeans(KMeans):\n",
        "    def train(self, hyperparameters, X, y=None):\n",
        "        # Validate and extract hyperparameters\n",
        "        K = hyperparameters.get('K')\n",
        "        tau = hyperparameters.get('tau')\n",
        "        max_iterations = hyperparameters.get('max_iterations')\n",
        "\n",
        "        if not isinstance(K, int) or K <= 0:\n",
        "            raise ValueError(\"Number of clusters K must be a positive integer.\")\n",
        "        if not isinstance(tau, (float, int)) or tau < 0:\n",
        "            raise ValueError(\"Convergence threshold tau must be a non-negative number.\")\n",
        "        if not isinstance(max_iterations, int) or max_iterations <= 0:\n",
        "            raise ValueError(\"Maximum iterations must be a positive integer.\")\n",
        "\n",
        "        # Get dataset dimensions\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Add a column for cluster assignments to X\n",
        "        X = np.hstack((X, np.zeros((n_samples, 1))))\n",
        "\n",
        "        # Initialize centroids (K x n_features)\n",
        "        centroids = np.random.rand(K, n_features)\n",
        "\n",
        "        # Initialize vector to count number of samples per cluster\n",
        "        cluster_sizes = np.zeros(K)\n",
        "\n",
        "        # Initialize the label vector and boolean mask for labeled samples\n",
        "        labeled_mask = np.zeros(n_samples, dtype=bool)\n",
        "\n",
        "        if y is not None:\n",
        "            for i in range(n_samples):\n",
        "                if y[i] != -1:  # Label assigned\n",
        "                    X[i, -1] = y[i]  # Assign label from y to X\n",
        "                    labeled_mask[i] = True  # Mark this sample as labeled\n",
        "                    cluster_sizes[int(y[i])] += 1  # Increment the cluster count\n",
        "\n",
        "        # Boolean variable to check convergence\n",
        "        convergence = False\n",
        "\n",
        "        # Iterative loop for K-Means\n",
        "        for iteration in range(max_iterations):\n",
        "            # Backup the current centroids\n",
        "            prev_centroids = centroids.copy()\n",
        "\n",
        "            # Reset centroids and cluster sizes\n",
        "            centroids = np.zeros((K, n_features))\n",
        "            cluster_sizes = np.zeros(K)\n",
        "\n",
        "            # Accumulate feature sums for each cluster, skipping labeled samples\n",
        "            for i in range(n_samples):\n",
        "                if labeled_mask[i]:  # Skip samples with labels already assigned\n",
        "                    continue\n",
        "                cluster = int(X[i, -1])  # Assigned cluster\n",
        "                centroids[cluster] += X[i, :-1]  # Add feature vector\n",
        "                cluster_sizes[cluster] += 1  # Increment cluster count\n",
        "\n",
        "            # Compute new centroids\n",
        "            for k in range(K):\n",
        "                if cluster_sizes[k] > 0:\n",
        "                    centroids[k] *= (1.0 / cluster_sizes[k])  # Average features\n",
        "\n",
        "            # Check for convergence\n",
        "            centroid_shift = np.linalg.norm(centroids - prev_centroids)\n",
        "            if centroid_shift < tau:\n",
        "                convergence = True\n",
        "                print(f\"Converged after {iteration + 1} iterations.\")\n",
        "                break\n",
        "\n",
        "        if not convergence:\n",
        "            print(f\"Reached maximum iterations: {max_iterations}.\")\n",
        "\n",
        "        return centroids, X[:, -1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v89MHinBxlab",
        "outputId": "223bc707-7725-4744-f629-db440c5575f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged after 2 iterations.\n",
            "Converged after 2 iterations.\n",
            "Centroids from K-Means: [[-0.04386439 -0.05971545]\n",
            " [-0.01432383 -0.06994851]\n",
            " [ 0.06182498  0.13776796]]\n",
            "Centroids from Semi-Supervised K-Means: [[0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikUNHIPwf8Zj",
        "outputId": "126137a8-b353-4a99-cf0f-e796174e1dd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing Data Standardization: (Inputs include mean: [0.48331931 0.57266763 0.46773823 0.53104806 0.51521381], std: [0.29231337 0.28721236 0.31493959 0.26748174 0.29296276])\n",
            "Performing Data Standardization: (Inputs include mean: [0.48331931 0.57266763 0.46773823 0.53104806 0.51521381], std: [0.29231337 0.28721236 0.31493959 0.26748174 0.29296276])\n",
            "Performing Data Undo Standardization: (Inputs include mean: [0.48331931 0.57266763 0.46773823 0.53104806 0.51521381], std: [0.29231337 0.28721236 0.31493959 0.26748174 0.29296276])\n",
            "Decompressed Data: [[0.79932575 0.54793866 0.4254587  0.25296237 0.8672916 ]\n",
            " [0.72641533 0.52250547 0.42233399 0.3519555  0.74578692]\n",
            " [0.56685095 0.41612752 0.39451781 0.62533458 0.41428127]\n",
            " [0.57121708 0.48133786 0.4210473  0.54816564 0.50392409]\n",
            " [0.97845885 0.31986713 0.31295499 0.33476638 0.7900372 ]\n",
            " [0.52097974 0.3435092  0.3691339  0.75094777 0.26461435]\n",
            " [0.42568307 0.75639119 0.54957583 0.38129928 0.68277475]\n",
            " [0.28053348 0.64184301 0.51691825 0.64987079 0.35822164]\n",
            " [0.94591282 0.18849316 0.26191703 0.51321128 0.58057652]\n",
            " [0.54202902 0.26698106 0.33534536 0.81618652 0.19122297]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class PCA:\n",
        "    def __init__(self, n_components):\n",
        "        self.n_components = n_components\n",
        "        self.mean_ = None\n",
        "        self.std_ = None\n",
        "        self.components_ = None\n",
        "\n",
        "    def _data_standardization(self, X, mean, std):\n",
        "        print(f\"Performing Data Standardization: (Inputs include mean: {mean}, std: {std})\")\n",
        "\n",
        "        # Step 1: Centering the data: subtract the mean from each feature\n",
        "        X_centered = X - mean\n",
        "\n",
        "        # Step 2: Inverse of the standard deviations: element-wise inverse of std\n",
        "        sigma_inverted = np.power(std, -1)  # Element-wise inverse of std\n",
        "\n",
        "        # Step 3: Create a diagonal matrix with the inverted standard deviations\n",
        "        sigma_inverted_diagonal = np.diag(sigma_inverted)\n",
        "\n",
        "        # Step 4: Multiply centered data by the diagonal matrix to scale the data\n",
        "        X_standardized = X_centered.dot(sigma_inverted_diagonal)\n",
        "\n",
        "        return X_standardized\n",
        "\n",
        "    def _data_undo_standardization(self, X_standardized, mean, std):\n",
        "\n",
        "        print(f\"Performing Data Undo Standardization: (Inputs include mean: {mean}, std: {std})\")\n",
        "\n",
        "        sigma_diagonal = np.diag(std)  # Create a diagonal matrix of the standard deviations\n",
        "        X_scaled = X_standardized.dot(sigma_diagonal)  # Multiply by the diagonal matrix\n",
        "\n",
        "        mean_tiled = np.tile(mean, (X_scaled.shape[0], 1))  # Tile the mean vector to match the number of rows in X_scaled\n",
        "        X_repositioned = X_scaled + mean_tiled  # Add the mean back to the scaled data\n",
        "\n",
        "        return X_repositioned\n",
        "\n",
        "    def train(self, X):\n",
        "        # Compute the mean and standard deviation per feature\n",
        "        self.mean_ = np.mean(X, axis=0)\n",
        "        self.std_ = np.std(X, axis=0)\n",
        "\n",
        "        # Standardize the data\n",
        "        X_standardized = self._data_standardization(X, self.mean_, self.std_)\n",
        "\n",
        "        # Compute the SVD for PCA\n",
        "        U, S, Vt = np.linalg.svd(X_standardized, full_matrices=False)\n",
        "        self.components_ = Vt[:self.n_components]\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Check if the model has been trained\n",
        "        if self.mean_ is None or self.std_ is None or self.components_ is None:\n",
        "            raise ValueError(\"Model has not been trained yet. Please train the model before prediction.\")\n",
        "\n",
        "        # Standardize the input X\n",
        "        X_standardized = self._data_standardization(X, self.mean_, self.std_)\n",
        "\n",
        "        # Data compression (multiply the learned parameter matrix by Xstandardized transpose)\n",
        "        Xcompressed_standardized = X_standardized.dot(self.components_.T)\n",
        "\n",
        "        # Data decompression (transpose the learned parameter matrix and multiply by compressed data)\n",
        "        Xdecompressed_standardized = Xcompressed_standardized.dot(self.components_)\n",
        "\n",
        "        # Ensure that Xdecompressed_standardized has the same shape as X_standardized\n",
        "        if Xdecompressed_standardized.shape != X_standardized.shape:\n",
        "            Xdecompressed_standardized = Xdecompressed_standardized.T\n",
        "\n",
        "        # Undo the standardization to return Xdecompressed in original units\n",
        "        Xdecompressed = self._data_undo_standardization(Xdecompressed_standardized, self.mean_, self.std_)\n",
        "\n",
        "        # Return the decompressed data\n",
        "        return Xdecompressed\n"
      ]
    }
  ]
}